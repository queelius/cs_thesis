<map version="1.0.1">
<!-- To view this file, download free mind mapping software FreeMind from http://freemind.sourceforge.net -->
<node CREATED="1392485300558" ID="ID_659508001" MODIFIED="1392485307039" TEXT="Research / survey">
<node CREATED="1392485308369" ID="ID_1545410602" MODIFIED="1393883085710" POSITION="right" TEXT="Information retrieval">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p style="font-weight: normal; letter-spacing: normal; word-spacing: 0px; font-variant: normal; text-indent: 0px; background-color: rgb(255, 255, 255); margin-right: 0px; text-align: start; font-style: normal; font-size: 13px; margin-left: 0px; margin-bottom: 0; text-transform: none; line-height: 19.200000762939453px; margin-top: 0; font-family: sans-serif; color: rgb(0, 0, 0); white-space: normal">
      <b>Information retrieval</b>&#160;is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on metadata or on full-text (or other content-based) indexing.
    </p>
    <p style="font-weight: normal; letter-spacing: normal; word-spacing: 0px; font-variant: normal; text-indent: 0px; background-color: rgb(255, 255, 255); margin-right: 0px; text-align: start; font-style: normal; font-size: 13px; margin-left: 0px; margin-bottom: 0; text-transform: none; line-height: 19.200000762939453px; margin-top: 0; font-family: sans-serif; color: rgb(0, 0, 0); white-space: normal">
      
    </p>
    <p style="font-weight: normal; letter-spacing: normal; word-spacing: 0px; font-variant: normal; text-indent: 0px; background-color: rgb(255, 255, 255); margin-right: 0px; text-align: start; font-style: normal; font-size: 13px; margin-left: 0px; margin-bottom: 0; text-transform: none; line-height: 19.200000762939453px; margin-top: 0; font-family: sans-serif; color: rgb(0, 0, 0); white-space: normal">
      Automated information retrieval systems are used to reduce what has been called &quot;<font color="rgb(11, 0, 128)"><a style="background-image: none; background-repeat: repeat; color: rgb(11, 0, 128); background-position: initial initial; text-decoration: none" href="http://en.wikipedia.org/wiki/Information_overload" title="Information overload">information overload</a></font>&quot;. Many universities and&#160;<font color="rgb(11, 0, 128)"><a style="background-image: none; background-repeat: repeat; color: rgb(11, 0, 128); background-position: initial initial; text-decoration: none" href="http://en.wikipedia.org/wiki/Public_library" title="Public library">public libraries</a></font>&#160;use IR systems to provide access to books, journals and other documents.&#160;<font color="rgb(11, 0, 128)"><a style="background-image: none; background-repeat: repeat; color: rgb(11, 0, 128); background-position: initial initial; text-decoration: none" href="http://en.wikipedia.org/wiki/Web_search_engine" title="Web search engine">Web search engines</a></font>&#160;are the most visible&#160;<font color="rgb(11, 0, 128)"><a style="background-image: none; background-repeat: repeat; color: rgb(11, 0, 128); background-position: initial initial; text-decoration: none" href="http://en.wikipedia.org/wiki/Information_retrieval_applications" title="Information retrieval applications">IR applications</a></font>.
    </p>
    <p class="MsoNormal">
      
    </p>
    <p class="MsoNormal">
      ---
    </p>
    <p class="MsoNormal">
      Information retrieval (IR) is &#64257;nding material (usually documents) of an unstructured nature (usually text) that satis&#64257;es an information need from within large collections (usually stored on computers).
    </p>
  </body>
</html></richcontent>
<node CREATED="1392485343889" ID="ID_1587813854" MODIFIED="1392485551065" TEXT="query">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <h2 style="font-weight: normal; letter-spacing: normal; font-style: normal; margin-top: 0px; line-height: 19.200000762939453px; margin-left: 0px; text-indent: 0px; font-size: 19px; padding-bottom: 0; padding-top: 0; font-variant: normal; text-align: start; white-space: normal; background-repeat: repeat; color: rgb(0, 0, 0); font-family: sans-serif; border-bottom-style: solid; margin-bottom: 0; border-bottom-width: 1px; background-image: none; background-position: initial initial; margin-right: 0px; border-bottom-color: rgb(170, 170, 170); background-color: rgb(255, 255, 255); text-transform: none; word-spacing: 0px">
      Overview[<font color="rgb(11, 0, 128)"><a style="text-decoration: none; background-repeat: repeat; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;action=edit&amp;section=1" title="Edit section: Overview">edit</a></font>]
    </h2>
    <p style="font-weight: normal; letter-spacing: normal; word-spacing: 0px; font-variant: normal; text-indent: 0px; background-color: rgb(255, 255, 255); margin-right: 0px; text-align: start; font-style: normal; font-size: 13px; margin-left: 0px; margin-bottom: 0; text-transform: none; line-height: 19.200000762939453px; margin-top: 0; white-space: normal; color: rgb(0, 0, 0); font-family: sans-serif">
      An information retrieval process begins when a user enters a&#160;<font color="rgb(11, 0, 128)"><a style="text-decoration: none; background-repeat: repeat; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Query_string" title="Query string">query</a></font>&#160;into the system. Queries are formal statements of&#160;<font color="rgb(11, 0, 128)"><a style="text-decoration: none; background-repeat: repeat; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Information_need" class="mw-redirect" title="Information need">information needs</a></font>, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of&#160;<font color="rgb(11, 0, 128)"><a style="text-decoration: none; background-repeat: repeat; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Relevance" title="Relevance">relevancy</a></font>.
    </p>
    <p style="font-weight: normal; letter-spacing: normal; word-spacing: 0px; font-variant: normal; text-indent: 0px; background-color: rgb(255, 255, 255); margin-right: 0px; text-align: start; font-style: normal; font-size: 13px; margin-left: 0px; margin-bottom: 0; text-transform: none; line-height: 19.200000762939453px; margin-top: 0; white-space: normal; color: rgb(0, 0, 0); font-family: sans-serif">
      An object is an entity that is represented by information in a&#160;<font color="rgb(11, 0, 128)"><a style="text-decoration: none; background-repeat: repeat; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Database" title="Database">database</a></font>. User queries are matched against the database information. Depending on the&#160;<font color="rgb(11, 0, 128)"><a style="text-decoration: none; background-repeat: repeat; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Information_retrieval_applications" title="Information retrieval applications">application</a></font>&#160;the data objects may be, for example, text documents, images,<font color="rgb(11, 0, 128)"><a style="text-decoration: none; background-repeat: repeat; white-space: nowrap; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Information_retrieval#cite_note-goodron2000-1"><sup style="font-weight: normal; font-style: normal; line-height: 1em" id="cite_ref-goodron2000_1-0" class="reference">[</sup></a><a style="text-decoration: none; background-repeat: repeat; white-space: nowrap; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Information_retrieval#cite_note-goodron2000-1"><sup style="font-weight: normal; font-style: normal; line-height: 1em" id="cite_ref-goodron2000_1-0" class="reference">1]</sup></a></font>&#160;audio,<font color="rgb(11, 0, 128)"><a style="text-decoration: none; background-repeat: repeat; white-space: nowrap; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Information_retrieval#cite_note-Foote99-2"><sup style="font-weight: normal; font-style: normal; line-height: 1em" id="cite_ref-Foote99_2-0" class="reference">[</sup></a><a style="text-decoration: none; background-repeat: repeat; white-space: nowrap; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Information_retrieval#cite_note-Foote99-2"><sup style="font-weight: normal; font-style: normal; line-height: 1em" id="cite_ref-Foote99_2-0" class="reference">2]</sup></a></font>&#160;<font color="rgb(11, 0, 128)"><a style="text-decoration: none; background-repeat: repeat; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Mind_maps" class="mw-redirect" title="Mind maps">mind maps</a><a style="text-decoration: none; background-repeat: repeat; white-space: nowrap; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Information_retrieval#cite_note-Beel2009-3"><sup style="font-weight: normal; font-style: normal; line-height: 1em" id="cite_ref-Beel2009_3-0" class="reference">[</sup></a><a style="text-decoration: none; background-repeat: repeat; white-space: nowrap; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Information_retrieval#cite_note-Beel2009-3"><sup style="font-weight: normal; font-style: normal; line-height: 1em" id="cite_ref-Beel2009_3-0" class="reference">3]</sup></a></font>&#160;or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.
    </p>
    <p style="font-weight: normal; letter-spacing: normal; word-spacing: 0px; font-variant: normal; text-indent: 0px; background-color: rgb(255, 255, 255); margin-right: 0px; text-align: start; font-style: normal; font-size: 13px; margin-left: 0px; margin-bottom: 0; text-transform: none; line-height: 19.200000762939453px; margin-top: 0; white-space: normal; color: rgb(0, 0, 0); font-family: sans-serif">
      Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.<font color="rgb(11, 0, 128)"><a style="text-decoration: none; background-repeat: repeat; white-space: nowrap; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Information_retrieval#cite_note-Frakes1992-4"><sup style="font-weight: normal; font-style: normal; line-height: 1em" id="cite_ref-Frakes1992_4-0" class="reference">[</sup></a><a style="text-decoration: none; background-repeat: repeat; white-space: nowrap; color: rgb(11, 0, 128); background-image: none; background-position: initial initial" href="http://en.wikipedia.org/wiki/Information_retrieval#cite_note-Frakes1992-4"><sup style="font-weight: normal; font-style: normal; line-height: 1em" id="cite_ref-Frakes1992_4-0" class="reference">4]</sup></a></font>
    </p>
  </body>
</html></richcontent>
<node CREATED="1392485552641" ID="ID_878322683" MODIFIED="1392485555150" TEXT="refinement"/>
<node CREATED="1392564860913" ID="ID_376488690" MODIFIED="1392564869566" TEXT="input into the system to satisfy information needs"/>
<node CREATED="1392565707505" ID="ID_1179516395" MODIFIED="1392565743649" TEXT="query language">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      <font color="rgb(0, 0, 0)" face="sans-serif" size="13px"><span style="display: inline !important; font-weight: normal; letter-spacing: normal; word-spacing: 0px; font-variant: normal; text-indent: 0px; background-color: rgb(255, 255, 255); text-align: start; font-style: normal; text-transform: none; font-size: 13px; float: none; line-height: 19.200000762939453px; font-family: sans-serif; color: rgb(0, 0, 0); white-space: normal">Information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry. </span></font>
    </p>
    <p>
      
    </p>
    <p>
      <font color="rgb(0, 0, 0)" size="13px" face="sans-serif"><span style="font-weight: normal; display: inline !important; letter-spacing: normal; word-spacing: 0px; text-indent: 0px; font-variant: normal; background-color: rgb(255, 255, 255); text-align: start; font-style: normal; text-transform: none; font-size: 13px; float: none; line-height: 19.200000762939453px; font-family: sans-serif; color: rgb(0, 0, 0); white-space: normal">This is in contrast to database query languages which attempt to give factual answers to factual questions.</span></font>
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392566653041" ID="ID_505001748" MODIFIED="1392566656014" TEXT="complex queries">
<node CREATED="1392566659137" ID="ID_1342297442" MODIFIED="1392566666014" TEXT="contextual query language (CQL)"/>
</node>
</node>
<node CREATED="1392485524241" ID="ID_303384083" MODIFIED="1392485525934" TEXT="rank"/>
<node CREATED="1392485558737" ID="ID_339181931" MODIFIED="1392485560414" TEXT="relevancy">
<node CREATED="1392495135265" ID="ID_319180952" MODIFIED="1392728004257" TEXT="testing performance">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      See: TREC
    </p>
  </body>
</html></richcontent>
<node CREATED="1392495141536" ID="ID_445093029" MODIFIED="1392495146302" TEXT="source of docs"/>
<node CREATED="1392495148753" ID="ID_469857069" MODIFIED="1392495150846" TEXT="source of queries"/>
<node CREATED="1392495152881" ID="ID_209943278" MODIFIED="1392495166734" TEXT="source of relevancy judgements for queries">
<node CREATED="1392495251999" ID="ID_1421528916" MODIFIED="1392495258206" TEXT="binary: relevant or not relevant"/>
</node>
</node>
</node>
<node CREATED="1392487934801" ID="ID_1133173469" MODIFIED="1392487944014" TEXT="performance measures">
<node CREATED="1392487944705" ID="ID_556395609" MODIFIED="1392574090939" TEXT="precision">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      http://en.wikipedia.org/wiki/Precision_(information_retrieval)
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392487948769" ID="ID_837103108" MODIFIED="1392574093782" TEXT="recall">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      http://en.wikipedia.org/wiki/Precision_(information_retrieval)
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392487951809" ID="ID_188155532" MODIFIED="1392487954398" TEXT="fall-out"/>
<node CREATED="1392487955584" ID="ID_216682480" MODIFIED="1392487958078" TEXT="F-measure"/>
<node CREATED="1392487964721" ID="ID_1436685876" MODIFIED="1392487969598" TEXT="average precision"/>
<node CREATED="1392489982209" ID="ID_1422207027" MODIFIED="1392574121992" TEXT="mean average precision">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      http://en.wikipedia.org/wiki/Mean_average_precision
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392574062689" ID="ID_342693641" MODIFIED="1392574064606" TEXT="Discounted cumulative gain"/>
</node>
<node CREATED="1392490312337" ID="ID_494861269" MODIFIED="1392490315598" TEXT="information overload">
<node CREATED="1392490721793" ID="ID_1617436963" MODIFIED="1392490732062" TEXT="relevant results important"/>
</node>
<node CREATED="1392664170449" ID="ID_602805058" MODIFIED="1392664176405" TEXT="term discrimination">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      http://en.wikipedia.org/wiki/Term_Discrimination
    </p>
  </body>
</html></richcontent>
</node>
</node>
<node CREATED="1392491201681" FOLDED="true" ID="ID_1405478875" MODIFIED="1393809585567" POSITION="left" TEXT="our research">
<node CREATED="1392491457969" ID="ID_142462762" MODIFIED="1393002787752" TEXT="bloom filter -&gt; signature">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      <span style="line-height: 16.799999237060547px; white-space: normal; font-variant: normal; text-indent: 0px; word-spacing: 0px; font-style: normal; letter-spacing: normal; color: rgb(0, 0, 0); display: inline !important; font-family: arial, sans-serif; text-transform: none; font-weight: normal; text-align: start; font-size: 12px; float: none"><font color="rgb(0, 0, 0)" face="arial, sans-serif" size="12px">A Bloomfilter is a simple space-efficient randomized data structure for representing a set in order to support membership queries. Bloom filters allow false positives but the space savings often outweigh this drawback when the probability of an error is controlled.</font></span>
    </p>
    <p>
      
    </p>
    <p>
      http://en.wikipedia.org/wiki/Hashing_trick
    </p>
  </body>
</html></richcontent>
<node CREATED="1392643593890" ID="ID_307181215" MODIFIED="1392644041184" TEXT="hash functions">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Another way to change the hash function is to map a character to a different value, e.g., a code lookup table. It's only going to have two columns (one for the original character, and one for the integer to map to), and Z rows, where Z is size of alphabet, e.g., a through z or a through z + 0 through 9, etc.
    </p>
    <p>
      
    </p>
    <p>
      See: Mapped Additive Shift Hashing (MASH) in paper &quot;Extremely Fast Text Feature Extraction for Classification and Indexing&quot;
    </p>
    <p>
      
    </p>
    <p>
      Now, we could have a single hash function, but change the lookup table. This is also easy to modify in the context of learning parameters, e.g., a stochastic gradient descent may try some slight tweaks to the table to see if it results in better performance (fewer false negatives for a given bloom filter size, or better spread among bloom filter indices such that its more difficult to gain information by looking at index values and hash functions).
    </p>
    <p>
      
    </p>
    <p>
      Maybe it's not even necessary to have more than one hash function? Learn good parameters for one hash function on the training data, and we're done? I don't know. It might be easier to gain information about the bloom filter though.
    </p>
  </body>
</html></richcontent>
<node CREATED="1392646369665" ID="ID_1732665722" MODIFIED="1392646373486" TEXT="false negatives"/>
<node CREATED="1392646375921" ID="ID_1180530242" MODIFIED="1392737406212" TEXT="distribution">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      In the context of bloom filters, distribution is this: for a set of words, does each word map to independent indices? If so, then a statistical analysis can be performed to infer properties of the document, even if each word unit (1-gram, 2-gram, ..., k-gram) in the document is pre-encrypted before hashed (pre-encrypted for query privacy).
    </p>
    <p>
      
    </p>
    <p>
      So, this is another performance metric to optimize: we want member indices to overlap to some degree while still maintaining a low probability on false positives. The hash functions, in this way, serve as a form of confidentiallity even without pre-encryption since you may not be able to infer much about what members (or how many) are present by looking at the bloom filter.
    </p>
  </body>
</html></richcontent>
</node>
</node>
</node>
<node CREATED="1392495034241" ID="ID_173779565" MODIFIED="1392633375906" TEXT="negative examples">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Use negative examples from a document repo. This may unfairly advantage my approach, but ...
    </p>
    <p>
      
    </p>
    <p>
      ---
    </p>
    <p>
      
    </p>
    <p>
      Co-occurrence can be extracted from document sources using ngram analysis.
    </p>
    <p>
      
    </p>
    <p>
      <font color="rgb(0, 0, 0)" face="sans-serif" size="13px"><span class="Apple-converted-space">Co-location can also be extracted from ngram analysis, e.g.: &#160;</span><span style="display: inline !important; font-weight: normal; letter-spacing: normal; word-spacing: 0px; font-variant: normal; text-indent: 0px; background-color: rgb(255, 255, 255); text-align: start; font-style: normal; text-transform: none; font-size: 13px; float: none; line-height: 19.200000762939453px; font-family: sans-serif; color: rgb(0, 0, 0); white-space: normal">'crystal clear', 'middle management', 'nuclear family', and 'cosmetic surgery'</span><span class="Apple-converted-space">&#160;</span></font>
    </p>
  </body>
</html></richcontent>
</node>
<node COLOR="#338800" CREATED="1392565117792" ID="ID_1611842458" MODIFIED="1392737319533" TEXT="query statistics">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      <font face="sans-serif" color="rgb(0, 0, 0)">From &quot;http://en.wikipedia.org/wiki/Web_search_query&quot; </font>
    </p>
    <p>
      
    </p>
    <p>
      <font face="sans-serif" color="rgb(0, 0, 0)">&quot;Research has shown that query term frequency distributions conform to the&#160;</font><font color="rgb(11, 0, 128)" face="sans-serif" size="13px"><a title="Power law" style="line-height: 19.200000762939453px; white-space: normal; font-variant: normal; background-image: none; text-indent: 0px; word-spacing: 0px; background-position: initial initial; font-style: normal; letter-spacing: normal; color: rgb(11, 0, 128); text-decoration: none; font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; font-size: 13px; background-color: rgb(255, 255, 255); background-repeat: repeat" href="http://en.wikipedia.org/wiki/Power_law">power law</a></font><font color="rgb(0, 0, 0)" face="sans-serif">, or&#160;</font><font color="rgb(0, 0, 0)" face="sans-serif" size="13px"><i style="white-space: normal; line-height: 19.200000762939453px; font-variant: normal; text-indent: 0px; word-spacing: 0px; letter-spacing: normal; color: rgb(0, 0, 0); font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; font-size: 13px; background-color: rgb(255, 255, 255)">long tail</i></font><font color="rgb(0, 0, 0)" face="sans-serif">&#160;distribution curves. That is, a small portion of the terms observed in a large query log (e.g. &gt; 100 million queries) are used most often, while the remaining terms are used less often individually.&quot; </font>
    </p>
    <p>
      
    </p>
    <p>
      Evidence in support of my supposition that to train the bloom filter, we can focus on training it to not respond with a false negative on probable queries.
    </p>
    <p>
      
    </p>
    <p>
      Source: http://link.springer.com/chapter/10.1007%2F978-3-540-31865-1_2
    </p>
    <p>
      
    </p>
    <p>
      However, also:
    </p>
    <p>
      
    </p>
    <p>
      <font color="rgb(0, 0, 0)" face="sans-serif">But in a recent study in 2011 it was found that the average length of queries has grown steadily over time and average length of non-English languages queries had increased more than English queries.<a style="white-space: nowrap; background-image: none; background-position: initial initial; color: rgb(11, 0, 128); text-decoration: none; background-repeat: repeat" href="http://en.wikipedia.org/wiki/Web_search_query#cite_note-8"><sup class="reference" style="white-space: normal; line-height: 1em; font-variant: normal; text-indent: 0px; word-spacing: 0px; font-style: normal; letter-spacing: normal; color: rgb(0, 0, 0); font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; background-color: rgb(255, 255, 255)" id="cite_ref-8">[</sup></a><a style="white-space: nowrap; background-image: none; background-position: initial initial; color: rgb(11, 0, 128); text-decoration: none; background-repeat: repeat" href="http://en.wikipedia.org/wiki/Web_search_query#cite_note-8"><sup class="reference" style="white-space: normal; line-height: 1em; font-variant: normal; text-indent: 0px; word-spacing: 0px; font-style: normal; letter-spacing: normal; color: rgb(0, 0, 0); font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; background-color: rgb(255, 255, 255)" id="cite_ref-8">8]</sup></a>For longer queries,&#160;</font><font color="rgb(11, 0, 128)" face="sans-serif" size="13px"><a title="Natural language processing" style="white-space: normal; line-height: 19.200000762939453px; font-variant: normal; text-indent: 0px; background-image: none; word-spacing: 0px; background-position: initial initial; font-style: normal; letter-spacing: normal; color: rgb(11, 0, 128); text-decoration: none; font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; font-size: 13px; background-color: rgb(255, 255, 255); background-repeat: repeat" href="http://en.wikipedia.org/wiki/Natural_language_processing">Natural language processing</a></font><font color="rgb(0, 0, 0)" face="sans-serif">&#160;helps, since parse trees of queries can be matched with that of answers and their snippets.<a style="white-space: nowrap; background-image: none; background-position: initial initial; color: rgb(11, 0, 128); text-decoration: none; background-repeat: repeat" href="http://en.wikipedia.org/wiki/Web_search_query#cite_note-9"><sup class="reference" style="white-space: normal; line-height: 1em; font-variant: normal; text-indent: 0px; word-spacing: 0px; font-style: normal; letter-spacing: normal; color: rgb(0, 0, 0); font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; background-color: rgb(255, 255, 255)" id="cite_ref-9">[</sup></a><a style="white-space: nowrap; background-image: none; background-position: initial initial; color: rgb(11, 0, 128); text-decoration: none; background-repeat: repeat" href="http://en.wikipedia.org/wiki/Web_search_query#cite_note-9"><sup class="reference" style="white-space: normal; line-height: 1em; font-variant: normal; text-indent: 0px; word-spacing: 0px; font-style: normal; letter-spacing: normal; color: rgb(0, 0, 0); font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; background-color: rgb(255, 255, 255)" id="cite_ref-9">9]</sup></a>&#160;For multi-sentence queries where keywords statistics and&#160;</font><font color="rgb(11, 0, 128)" face="sans-serif" size="13px"><a title="Tf&#x2013;idf" style="white-space: normal; line-height: 19.200000762939453px; font-variant: normal; text-indent: 0px; background-image: none; word-spacing: 0px; background-position: initial initial; font-style: normal; letter-spacing: normal; color: rgb(11, 0, 128); text-decoration: none; font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; font-size: 13px; background-color: rgb(255, 255, 255); background-repeat: repeat" href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">Tf&#8211;idf</a></font><font color="rgb(0, 0, 0)" face="sans-serif">&#160;is not very helpful,&#160;</font><font color="rgb(11, 0, 128)" face="sans-serif" size="13px"><a class="mw-redirect" title="Parse thicket" style="white-space: normal; line-height: 19.200000762939453px; font-variant: normal; text-indent: 0px; background-image: none; word-spacing: 0px; background-position: initial initial; font-style: normal; letter-spacing: normal; color: rgb(11, 0, 128); text-decoration: none; font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; font-size: 13px; background-color: rgb(255, 255, 255); background-repeat: repeat" href="http://en.wikipedia.org/wiki/Parse_thicket">Parse thicket</a></font><font color="rgb(0, 0, 0)" face="sans-serif">&#160;technique comes into play to structurally represent complex questions and answers.<a style="white-space: nowrap; background-image: none; background-position: initial initial; color: rgb(11, 0, 128); text-decoration: none; background-repeat: repeat" href="http://en.wikipedia.org/wiki/Web_search_query#cite_note-10"><sup class="reference" style="white-space: normal; line-height: 1em; font-variant: normal; text-indent: 0px; word-spacing: 0px; font-style: normal; letter-spacing: normal; color: rgb(0, 0, 0); font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; background-color: rgb(255, 255, 255)" id="cite_ref-10">[</sup></a><a style="white-space: nowrap; background-image: none; background-position: initial initial; color: rgb(11, 0, 128); text-decoration: none; background-repeat: repeat" href="http://en.wikipedia.org/wiki/Web_search_query#cite_note-10"><sup class="reference" style="white-space: normal; line-height: 1em; font-variant: normal; text-indent: 0px; word-spacing: 0px; font-style: normal; letter-spacing: normal; color: rgb(0, 0, 0); font-family: sans-serif; text-transform: none; font-weight: normal; text-align: start; background-color: rgb(255, 255, 255)" id="cite_ref-10">10]</sup></a></font>
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392565638801" ID="ID_5080407" MODIFIED="1392565676875" TEXT="vertical search?">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      If vertical search, can learn more effective bloom filters?
    </p>
    <p>
      
    </p>
    <p>
      http://en.wikipedia.org/wiki/Vertical_search
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392565886191" ID="ID_1642493273" MODIFIED="1392571971304" TEXT="relevancy and visualization">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      A form of relevance feedback: http://en.wikipedia.org/wiki/Relevance_feedback
    </p>
    <p>
      
    </p>
    <p>
      Given a document store D, which documents are most relevant to the information need of the user? (Information need is extracted from the query.) Return a set of documents, d, ranked by relevancy that satisfy the query, e.g., tf-idf or other keyword weighting approaches.
    </p>
    <p>
      
    </p>
    <p>
      Once we have a set of ranked documents, visualization is a further refinement to satisfy the information need of the user. This refinement stage is especially important in the context of our research since the document itself may not be immediately accessible (encrypted), and acquiring access may be costly (declassification). Thus, this visualization step may be important as the user can consider what he's looking for and visually pick out candidates that seem like a better match towards his potentially ill-defined information needs.
    </p>
    <p>
      
    </p>
    <p>
      This visualization step can also serve to refine his information need to a specific block within a given document. Since the document itself may be sensitive (e.g., trade secrets of a company or classified government documents), being granted access to the entire document may be a hard sell. It may be an easier sell if the user only asks for access to some subset of the document, e.g., 4 blocks (blocks 3 to 7) out of 200 blocks.
    </p>
  </body>
</html></richcontent>
<node CREATED="1392573608689" ID="ID_488908713" MODIFIED="1392573613726" TEXT="refinements">
<node CREATED="1392573620881" ID="ID_154615010" MODIFIED="1392573999838" TEXT="tweaking keyword weights">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Let the user tweak the keyword weights while visualizing relevancy to try to narrow in on fewer blocks.
    </p>
    <p>
      
    </p>
    <p>
      This is based on Rocchio algorithm's concept of re-weighting the terms in a vector space model. Basically, it's a form of nearest centroid classification, in which some of the documents are classified as belonging to the class (relevant to query) and others not. Then, move the centroid (or centroids) around to change the class. Move them in a direction guided by the user such that, through refinement by the user, a different set of documents compared to the initial set is included in the result set.
    </p>
  </body>
</html></richcontent>
</node>
</node>
</node>
<node CREATED="1392572420449" ID="ID_968474872" MODIFIED="1393809568120" TEXT="applicable?">
<node CREATED="1392572264241" ID="ID_1546749616" MODIFIED="1392573494739" TEXT="query expansion">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Query expansion is designed to increase the recall of the set of documents retrieved. That is, return more of the documents relevant to the user's information need. This may reduce the precision, e.g., ratio of number of relevant documents retrieved to the number of documents retrieved.
    </p>
    <p>
      
    </p>
    <p>
      Difficult is more difficult to apply to our work since query expansion often relies upon words near to the query words in the source document, but since the source document is concealed (we do not see the document as a bag of words; rather, the document is like a hidden bag of words, and we only know a word is in the bag if we guess a word, and then we are told whether said word is in the bag. That said, we can still do query expansions like if query = &quot;hello world&quot; then an expansion might be &quot;(hello OR hi OR ... OR greetings) AND (world OR planet OR ... OR earth). Of course, now we must be careful about not expanding it to terms which have the wrong sense; this is a natural language problem, e.g., sense disambiguation.
    </p>
    <p>
      
    </p>
    <p>
      With regards to being more difficult to apply: this is both an advantage and a disadvantage. It is a disadvantage in the context of satisfying a user's information needs; it is an advantage in the context of satisfying a document's security needs. So, a trade-off.
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392572431073" ID="ID_1072907235" MODIFIED="1392573161005" TEXT="vector space model">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Same reasons as for query expansion. Vector space model represents document and query as a vector, and then does simple linear algebra (e.g., angle difference) on them to score the relevance of a given document to a given query.
    </p>
    <p>
      
    </p>
    <p>
      We do not have a bag of words in our bloom filter readily available. We have only a yes/no answer to whether a given term is a member of a bloom filter. We can, of course, do a bunch of such queries to CONSTRUCT an approximate bag of words and then apply the vector space model to the problem. However, this approach will miss terms (especially non-common terms which a dictionary wouldn't include, for instance) and, of course, falsely include terms that shouldn't be present.
    </p>
    <p>
      
    </p>
    <p>
      More over, this is a slow way to do it. Every query is now expanded to N-queries, where N is extremely large.
    </p>
    <p>
      
    </p>
    <p>
      It is, however, reasonable to construct a vector from the terms in a query, e.g., if a query is &quot;hello world&quot; and the document contains &quot;hello&quot;, then:
    </p>
    <p>
      
    </p>
    <p>
      query vector = &lt;hello=1, world=1&gt; = &lt;1,1&gt;, and document vector = &lt;hello=1,vector=0&gt;=&lt;1,0&gt;. Angle difference is, then, 45 degrees. Furthermore, we can weight the terms, e.g., if world is given a weight twice as large as hello, then:
    </p>
    <p>
      
    </p>
    <p>
      &lt;hello=1, world=2&gt; * &lt;hello=1, world=0&gt;, then angle is 63 degrees. This is larger than 45 degrees, which makes sense since we're weighing &quot;world&quot; more than &quot;hello&quot;, and the document doesn't contain &quot;world&quot;.
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392565018865" ID="ID_1746676888" MODIFIED="1392565023006" TEXT="navigational queries">
<node CREATED="1392565030465" ID="ID_1173372445" MODIFIED="1392565039358" TEXT="not applicable to our research?"/>
</node>
</node>
<node CREATED="1392595010320" ID="ID_889922492" MODIFIED="1392737309259" TEXT="probability of inferring words from bloom filter indices">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      If the hash functions do not uniformly distribute words to indices, e.g., some words have indices all to themselves, then this is a vulnerability in our solution. During training, we an also check to make sure that no word in the training set maps to indices that no other word in the training set maps to, i.e., there is no overlap.
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392596795649" ID="ID_1498315175" MODIFIED="1392596798398" TEXT="preprocessing">
<node CREATED="1392596799505" ID="ID_335017681" MODIFIED="1392596804188" TEXT="semantic compression"/>
</node>
<node CREATED="1392477838086" ID="ID_363758109" MODIFIED="1392696038157" TEXT="hybrid">
<node CREATED="1392477845503" ID="ID_565790836" MODIFIED="1392477857214" TEXT="bloom filter for quick dirty lookups">
<node CREATED="1392477858161" ID="ID_567985122" MODIFIED="1392477877486" TEXT="if a hit occurs">
<node CREATED="1392477878209" ID="ID_1699809600" MODIFIED="1392477882542" TEXT="use conventional lookup"/>
<node CREATED="1392477907489" ID="ID_178863178" MODIFIED="1392477910622" TEXT="could be a false positive"/>
<node CREATED="1392477959234" ID="ID_559485933" MODIFIED="1392477971246" TEXT="maybe need to request full document or index?"/>
</node>
<node CREATED="1392477885025" ID="ID_463468340" MODIFIED="1392477888638" TEXT="if no hit occurs">
<node CREATED="1392477889153" ID="ID_20877592" MODIFIED="1392477900846" TEXT="bloom filters: no false negatives"/>
<node CREATED="1392477902721" ID="ID_1024043518" MODIFIED="1392477904894" TEXT="ignore"/>
</node>
</node>
<node CREATED="1392586956209" ID="ID_3101480" MODIFIED="1392701421097" TEXT="other approaches">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Take the words in the document, then encrypt them individually. You could expose this bag of encrypted words.
    </p>
    <p>
      
    </p>
    <p>
      Problems:
    </p>
    <p>
      
    </p>
    <p>
      (1) Frequency analysis. You can determine which encrypted words are most common.
    </p>
    <p>
      
    </p>
    <p>
      (2) Only keyword matching. Approximate matching is expensive (try variations of a keyword, see if it matches). Also, cannot do multiple-keyword (n-gram) matching unless you also encrypt those. Again, expensive, and variations are now more problematic.
    </p>
    <p>
      
    </p>
    <p>
      (3) You can only search it if you have the key. What if you want anyone to be able to search it?
    </p>
    <p>
      
    </p>
    <p>
      (4) Sequential search. Unless you create an inverted index on these encrypted words.
    </p>
    <p>
      
    </p>
    <p>
      Solutions?
    </p>
    <p>
      
    </p>
    <p>
      ... this text is obsolete I think. Revisit.
    </p>
  </body>
</html></richcontent>
</node>
</node>
<node CREATED="1392728090049" ID="ID_1451103273" MODIFIED="1392737167374" TEXT="new big ideas">
<node CREATED="1392728094146" ID="ID_109736224" MODIFIED="1392728192664" TEXT="limited regular expressions">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      A limited form of &quot;regular expression&quot; searching. E.g., have two classes, # or &amp; for numeric or alphabetical respectively. A third class is * for alphanumeric. Three classes, then. Ok, so, for each character position in a term, insert an '&amp;' and '*' in addition to the original character if the character is alphabetical, and insert '#' and '*' and the original character (numeric) if the character is numeric. That means, for a term with k characters, there are 3^k variations. This is fairly poor behavior, e.g., 3 character words have 3^3=27 variations, 4 -&gt; 81, 5 -&gt; 243, ..., 10 -&gt; 59049.&#160;&#160;If we restrict it to just one class, * for any character, then we instead have 3-&gt;8, 4-&gt;16, 5-&gt;32, ..., 10-&gt;1024. More reasonable, but still grows quickly. As long as k is reasonably small, we may be able to train a bloom filter effectively. Of course, we can also just tolerate a very high probability of a false positive on regular expression searching; after all, if the user fails to find what he wants with a normal search, then the regular expression search may be used as a last ditch effort, where higher false probabilities may become more acceptable in turn for improved access.
    </p>
    <p>
      
    </p>
    <p>
      Another alternative is to limit the number of variations, e.g., a term can only include a maximum of k *'s. So, now we've reduced the problem down to, for a term with n characters, (n choose k) * 2^k. This is much more reasonable, and still provides good regular expression matching.
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392728114402" ID="ID_1885785586" MODIFIED="1392737170003" TEXT="one-way pre-hashing w/secret">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Encrypt terms before inserting into bloom filter. This protects query privacy in addition to data privacy. Another solution to query privacy: a one-way hash. Faster, secure if we assume the one-way hash has the property that it is hard to discover a string which has the given hash value (pre-image resistance). If we use a one-way hash, and if the algorithm (for hashing) is known, then it is easy for common words to be discovered. A solution to this is a secret, so we hash instead OneWayHash(keyword | secret). This is fast and secure as long as secret is secure.
    </p>
    <p>
      
    </p>
    <p>
      If secret is revealed...what to do? Bloom filter is already constructed using the secret. There is a solution not involving reencryption in the paper &quot;Shared and Searchable Encrypted Data for Untrusted Servers&quot;, but it seems overly complex and has already been done. I trivial solution is to remake the bloom filters with a new secret, but this is admittedly expensive -- the one-way hash may be fast, but relearning the bloom filter is problematic. Maybe, however, all we need to do is remember the properties of the bloom filter -- hash functions and size -- and just remake them with the same properties but a different secret. Since the one-way hash is fast, the whole operation may be fast. But, this undermines the need for learning... or does it?
    </p>
    <p>
      
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392728143313" ID="ID_1549441524" MODIFIED="1392737147546" TEXT="subset checks to reduce false positives">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      For a compound term &quot;A B&quot; to be in the bloom filter, terms &quot;A&quot; and &quot;B&quot; must independently be in the filter. For a compound term &quot;A B C&quot; to be in the bloom filter, terms &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A B&quot;, &quot;B C&quot; must independently be in the bloom filter. In this way, we can gain confidence in a compound terms' membership by checking if some sampling of subsets are present too. If not, we know &quot;A B C&quot; is a false positive. If all subset checks are true, then &quot;A B C&quot; may still be a false positive, but the probability of it being a false positive has diminished. I believe I can formalize this exactly using mathematics.
    </p>
    <p>
      
    </p>
    <p>
      k = number of hash functions
    </p>
    <p>
      n = number of members inserted
    </p>
    <p>
      m = size of bloom filter (array size)
    </p>
    <p>
      r = number of non-present sub-grams of the n-gram, e.g., if we're talking about a 3-gram &quot;a b c&quot;, then there are {&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a b&quot;, &quot;b c&quot;, &quot;a b c&quot;} subgrams to check, of those some may not be present. Let's call that r. At a maximum, there are n + (n - 1) + ... + 1 = n(n+1)/2 sub-grams of an n-gram to check.
    </p>
    <p>
      
    </p>
    <p>
      P[n-gram is a false positive] = P[all non-strict subsets are false positives] = {[1 - (1 - 1/m))^(kn)]^k}^r ~&#160;[(1 - e^{-kn/m})^k]^r = (1 - e^{-kn/m})^(kr). When we don't do the subset checks, probability of a false positive is just&#160;&#160;(1 - e^{-kn/m})^k, so what's the ratio of these two equations?
    </p>
    <p>
      
    </p>
    <p>
      (1-e^{-kn/m})^{k(1-r)}. Note that 0 &lt; (1 - e^{-kn/m}) &lt; 1, so let u = (1 - e^{-kn/m}) =&gt; u^k(1-r). u is just the false positive probability, so let's rewrite that as a fraction, a/100 =&gt; (100/a)^{k(r-1)}. Let a = 1 (1% false positive probability), then 100^{k(r-1)}. This is how much more likely we'll have a false positive on an n-gram query without sub-gram checking as opposed to an n-gram query with subgram checking where there are r negative subsets that can be checked. Sometimes, this can be quite high, but let's say there is on average only 2 negative subsets for an n-gram query. That's extremely conservative for large n. Then, 100^k more likely to have a false positive without checking! Let's say there are 2 hash functions, 100^2! So, we can definitely exploit this property. As a sanity check on math, if there is only 1 subset, that means only the n-gram itself is a negative example and the n(n+1)/2 subgrams are positive examples, then math reduces to 100^{k*0} = 1, which is correct -- no advantage given at this point since there are no subgrams to check to reinforce a positive as a true positive.
    </p>
    <p>
      
    </p>
    <p>
      This is a significant reduction in probability of a false positive. We can use this to have extremely high confidence in a true positive the larger the n-gram granularity becomes, and so to meet a given target false probability for each n-gram size, we can keep increasing the false positive probability in a certain way, which means the size of the bloom filter can increase very slowly as we add more and more n-grams. I'm thinking we can have very large ngrams without much little adverse effect on bloom filter array size for a given probability of a false positive.
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392737193377" ID="ID_872235637" MODIFIED="1392740186618" TEXT="new fitness parameter">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      To give uncertainty to wether a word is a true positive, we may want to not MINIMIZE false probability, but to make it as close as possible to a certain value for P[tp]. P[tp] = P[+] - P[fp].
    </p>
  </body>
</html></richcontent>
<node CREATED="1392725701585" ID="ID_306356400" MODIFIED="1392727936459" TEXT="why a bloom filter?">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      We hash each word or n-gram (these are features; thus this is a form of dimensionality reduction]. So, we don't care what it hashes to, we'll just make that hash index true if we insert the given word.
    </p>
    <p>
      
    </p>
    <p>
      Now, if we only used one hash function, what's the problem? Let's consider the fact that we don't care about collisions among members, and collisions among non-members. In fact, as a form of dimensionality reduction and space savings, we ought to look for a hash function that maximizes these sort of collisions. A perfect hash function in fact hashes all members to index 0, and hashes all non-members to index 1 (or at least most nonmembers). This is a form of drastic dimensionality reduction; it is indeed a binary classifier using the simplest possible representation. This is good! A billion word document, for querying purposes, is just 2 bits and a hash function of size M bits, so 2 + M bits in total. But notice that M must be factored into this equation too.
    </p>
    <p>
      
    </p>
    <p>
      For instance, if the hash function is:
    </p>
    <p>
      
    </p>
    <p>
      hash(word) -&gt; index
    </p>
    <p>
      &#160;&#160;&#160;&#160;return 0 if word is in table[world]
    </p>
    <p>
      &#160;&#160;&#160;&#160;else return 1
    </p>
    <p>
      
    </p>
    <p>
      Then we haven't really accomplished anything; we're just using a massive lookup table. The complexity of the document is now represented in a table and so M = O(f(size of document)), where f is probably the square root function (according to zipf's law). The only savings we had was a product of the fact that some words repeat, and we can use that to do a form of simple compression. Moreover, the lookup table (or in general, hash function) itself may have other undesirable properties, like revealing the words (no data confidentiality) of the document. So, we're looking for a hash function that can be represented, say, as a matrix or vector of weight coefficients that operate on, say, the characters of the word [although we can operate on any feature of the words/ngrams, e.g., let the weight vector apply to n bits of a word at a time], e.g.:
    </p>
    <p>
      
    </p>
    <p>
      hash(word) -&gt; index
    </p>
    <p>
      &#160;&#160;&#160;&#160;sum &lt;- 0
    </p>
    <p>
      &#160;&#160;&#160;&#160;for i &lt;- 0 to size(word):
    </p>
    <p>
      &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;sum &lt;- sum + a^i * weight[word[i]]
    </p>
    <p>
      
    </p>
    <p>
      Unfortunately, the perfect hash function described earlier isn't practical on any realistic document. So, instead, what'll happen is the words (or ngrams) of a document will be spread around to different indexes, sometimes colliding and sometimes not colliding. We want to make it so that the probability that a word not in the document will collide with a word in the document (a false positive) is negligible. If we just make the size of the array (number of indices) really large, say 2^20, this is somewhat trivial to do. In fact, for any given bag of words much smaller than 2^20, they'll all probably map to different indices and be spread out quite far at that. But, this has two primary disadvantages:
    </p>
    <p>
      
    </p>
    <p>
      (1) Takes up a lot of space. In the example, 2^20 bits, in fact, which is 2^20 / 2^3 = 2^17 bytes. Ok, less than a megabyte.
    </p>
    <p>
      (2) If you desire privacy, then since for any given bag of words much smaller than 2^20, each word has a high probability of mapping to a unique index. So, if a word maps to a true index, that means the person can be confident that the word is there. In fact, he can just look at the distribution inside the array and infer a lot of stats about your data, e.g., number of unique words.
    </p>
    <p>
      
    </p>
    <p>
      So, in this sense, collisions is a privacy inducing feature: if two words map to the same index, you don't if the document contains one of the words, or both of the words, or maybe any of the words since, of course, another word could have mapped there (but the last point is the case with the trivial solution also).
    </p>
    <p>
      
    </p>
    <p>
      So what do we want to do? We want to keep the size of the array kind of small, and we want to maximize collisions between members, and separately maximize collisions among nonmembers. This will be hard to do with just one function -- maybe! I think we need more degrees of freedom. This is where bloom filters come into play. There's a lot of background theory for bloom filters, e.g., how large it, as a statistical expectation, needs to be&#160;&#160;to be under a given probability of a false positive for a given number of hash functions, etc. But I think we can do a lot better than these expectations since we'll be training our bloom filters more intelligently to discriminate between members and nonmembers using probabilities mined from corpora.
    </p>
    <p>
      
    </p>
    <p>
      When learning a bloom filter in this way, I will start off with one function. But I won't just be trying to minimize probability of a false positive, I'll also be trying to maximize collisions to make it impossible to tell which words are in a document. You can still do some analysis, granted, like a bloom filter with H hashes and size M with N &lt;= M true indices has an expected number of unique members, and they can still use sophisticated algorithms to discern some probability of which words are in it (that is why it's useful in the first place! otherwise we may as well spit out a 0.5 probability of a given word being in the filter).
    </p>
    <p>
      
    </p>
    <p>
      However, even this information leakage can be reduced if we do the following: use a one-way hash with a secret and hash that. Now, some stats can still be gleamed from the bloom filter distribution, but not much! Maybe the only stat: expected number of unique elements in the bloom filter. But I suspect even this information can be minimized, or given huge error bars, through training the bloom filter to penalize a bloom filter that doesn't have huge error bars on this.
    </p>
  </body>
</html></richcontent>
</node>
</node>
</node>
<node CREATED="1393001545641" ID="ID_1321305081" MODIFIED="1393002773078" TEXT="deterministic bit sets">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      For small sets, bloom filters are less effective than bit arrays. Thus, we're interested in blowing up the sets (adding ngrams, adding 1-edit errors, adding limited regular expressions); the trade-off is we'll have false positives for some subset of inputs.
    </p>
    <p>
      
    </p>
    <p>
      So, what subset do we want to bias the bloom filter to give false positives on? This is covered by our training. We want it to learn hash functions (at a given bloom filter size) which reduce the probability of a false positive on likely ngram queries. This gets back to the fact that language is very regular. What data set do we wish to use for this?
    </p>
    <p>
      
    </p>
    <p>
      Also, if we used a classical deterministic bit set, there is the question of security. Can someone look at your bit set and do some sort of analysis? They know exactly how many unique words are in the document/block, for instance; just count the number of bit positions that are set. This may or may not be useful.
    </p>
  </body>
</html></richcontent>
</node>
</node>
<node CREATED="1392491484529" ID="ID_1010151787" MODIFIED="1393883086990" POSITION="right" TEXT="document retrieval">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <h2 style="word-spacing: 0px; text-indent: 0px; font-size: 19px; white-space: normal; margin-top: 0px; text-transform: none; border-bottom-style: solid; border-bottom-color: rgb(170, 170, 170); font-weight: normal; margin-right: 0px; font-style: normal; padding-bottom: 0; border-bottom-width: 1px; letter-spacing: normal; padding-top: 0; color: rgb(0, 0, 0); background-position: initial initial; line-height: 19.200000762939453px; background-repeat: repeat; margin-left: 0px; font-family: sans-serif; background-image: none; text-align: start; background-color: rgb(255, 255, 255); font-variant: normal; margin-bottom: 0">
      Description[<font color="rgb(11, 0, 128)"><a title="Edit section: Description" style="text-decoration: none; color: rgb(11, 0, 128); background-position: initial initial; background-repeat: repeat; background-image: none" href="http://en.wikipedia.org/w/index.php?title=Document_retrieval&amp;action=edit&amp;section=1">edit</a></font>]
    </h2>
    <p style="line-height: 19.200000762939453px; white-space: normal; font-variant: normal; text-indent: 0px; word-spacing: 0px; font-style: normal; letter-spacing: normal; color: rgb(0, 0, 0); margin-right: 0px; margin-bottom: 0; font-family: sans-serif; text-transform: none; font-weight: normal; font-size: 13px; text-align: start; background-color: rgb(255, 255, 255); margin-top: 0; margin-left: 0px">
      Document retrieval systems find information to given criteria by matching text records (<i>documents</i>) against user queries, as opposed to&#160;<font color="rgb(11, 0, 128)"><a title="Expert system" style="text-decoration: none; color: rgb(11, 0, 128); background-position: initial initial; background-repeat: repeat; background-image: none" href="http://en.wikipedia.org/wiki/Expert_system">expert systems</a></font>&#160;that answer questions by&#160;<font color="rgb(11, 0, 128)"><a title="Inference" style="text-decoration: none; color: rgb(11, 0, 128); background-position: initial initial; background-repeat: repeat; background-image: none" href="http://en.wikipedia.org/wiki/Inference">inferring</a></font>&#160;over a logical knowledge database. A document retrieval system consists of a database of documents, a classification algorithm to build a full text index, and a user interface to access the database.
    </p>
    <p style="line-height: 19.200000762939453px; white-space: normal; font-variant: normal; text-indent: 0px; word-spacing: 0px; font-style: normal; letter-spacing: normal; color: rgb(0, 0, 0); margin-right: 0px; margin-bottom: 0; font-family: sans-serif; text-transform: none; font-weight: normal; font-size: 13px; text-align: start; background-color: rgb(255, 255, 255); margin-top: 0; margin-left: 0px">
      A document retrieval system has two main tasks:
    </p>
    <ol style="word-spacing: 0px; padding-left: 0px; text-indent: 0px; font-size: 13px; white-space: normal; margin-top: 0; text-transform: none; font-weight: normal; margin-right: 0px; font-style: normal; padding-bottom: 0px; list-style-image: none; letter-spacing: normal; padding-top: 0px; color: rgb(0, 0, 0); line-height: 19.200000762939453px; margin-left: 0; font-family: sans-serif; background-color: rgb(255, 255, 255); text-align: start; font-variant: normal; padding-right: 0px; margin-bottom: 0px">
      <li style="margin-bottom: 0">
        Find relevant documents to user queries
      </li>
      <li style="margin-bottom: 0">
        Evaluate the matching results and sort them according to relevance, using algorithms such as&#160;<font color="rgb(11, 0, 128)"><a title="PageRank" style="text-decoration: none; color: rgb(11, 0, 128); background-position: initial initial; background-repeat: repeat; background-image: none" href="http://en.wikipedia.org/wiki/PageRank">PageRank</a></font>.
      </li>
    </ol>
    <p style="line-height: 19.200000762939453px; white-space: normal; font-variant: normal; text-indent: 0px; word-spacing: 0px; font-style: normal; letter-spacing: normal; color: rgb(0, 0, 0); margin-right: 0px; margin-bottom: 0; font-family: sans-serif; text-transform: none; font-weight: normal; font-size: 13px; text-align: start; background-color: rgb(255, 255, 255); margin-top: 0; margin-left: 0px">
      Internet&#160;<font color="rgb(11, 0, 128)"><a class="mw-redirect" title="Search engines" style="text-decoration: none; color: rgb(11, 0, 128); background-position: initial initial; background-repeat: repeat; background-image: none" href="http://en.wikipedia.org/wiki/Search_engines">search engines</a></font>&#160;are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using&#160;<font color="rgb(11, 0, 128)"><a class="mw-redirect" title="Statistical" style="text-decoration: none; color: rgb(11, 0, 128); background-position: initial initial; background-repeat: repeat; background-image: none" href="http://en.wikipedia.org/wiki/Statistical">statistical</a></font>&#160;or<font color="rgb(11, 0, 128)"><a title="Natural language processing" style="text-decoration: none; color: rgb(11, 0, 128); background-position: initial initial; background-repeat: repeat; background-image: none" href="http://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a></font>&#160;techniques.
    </p>
  </body>
</html></richcontent>
<node CREATED="1392491543633" ID="ID_153364293" MODIFIED="1392491547070" TEXT="text retrieval"/>
<node CREATED="1392491689361" ID="ID_1719054604" MODIFIED="1392491694846" TEXT="two main tasks">
<node CREATED="1392491695937" ID="ID_1233595963" MODIFIED="1392491702926" TEXT="(1) find relevant docs"/>
<node CREATED="1392491704385" ID="ID_1442087061" MODIFIED="1392491747278" TEXT="(2) rank results (relevancy)">
<node CREATED="1392491552992" ID="ID_481419726" MODIFIED="1392491554734" TEXT="page rank"/>
</node>
</node>
<node CREATED="1392491757777" ID="ID_1609522415" MODIFIED="1392491763198" TEXT="internet search"/>
<node CREATED="1392491797761" ID="ID_1713454924" MODIFIED="1392491804590" TEXT="kinds of">
<node CREATED="1392491805825" ID="ID_451360823" MODIFIED="1392491816654" TEXT="form-based (syntactic)">
<node CREATED="1392491822016" ID="ID_1218751503" MODIFIED="1392491826846" TEXT="substring matching"/>
<node CREATED="1392491843664" ID="ID_1893461241" MODIFIED="1392491846382" TEXT="suffix tree"/>
</node>
</node>
</node>
<node CREATED="1392558806658" ID="ID_282657893" MODIFIED="1393883087941" POSITION="right" TEXT="vector space model">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Advantages (over standard boolean model)
    </p>
    <p>
      &#160;&#160;&#160;&#160;* simple model based on linear algebra
    </p>
    <p>
      &#160;&#160;&#160;&#160;* allows computing degree of similarity between queries and documents
    </p>
    <p>
      &#160;&#160;&#160;&#160;* allows ranking of documents by relevance
    </p>
    <p>
      &#160;&#160;&#160;&#160;* partial matching (approximate)
    </p>
    <p>
      
    </p>
    <p>
      Limitations:
    </p>
    <p>
      &#160;&#160;&#160;&#160;* long documents poorly represented
    </p>
    <p>
      &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;- similarity is low -- small scalar and curse of dimensionality
    </p>
    <p>
      &#160;&#160;&#160;&#160;* query terms must precisely match document terms
    </p>
    <p>
      &#160;&#160;&#160;&#160;* semantically insensitive -- docs are just a bag of terms (words) so synonymy and polyonomy&#160;is a problem -- false negative
    </p>
    <p>
      &#160;&#160;&#160;&#160;* order of words is lost in the representation
    </p>
    <p>
      &#160;&#160;&#160;&#160;* assumes terms are statistically independent
    </p>
    <p>
      &#160;&#160;&#160;&#160;* weighting of terms is not very formal
    </p>
    <p>
      
    </p>
  </body>
</html></richcontent>
<node CREATED="1392492156097" ID="ID_498352524" MODIFIED="1392492238179" TEXT="cosine similarity">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      On the bloom filter, can only do this:
    </p>
    <p>
      
    </p>
    <p>
      Take a query q, transform it to q', note that at this point query q' is a match (it may be a false positive, however). Then, take cosine similarity of q and q'.
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392558944959" ID="ID_716858160" MODIFIED="1392563976711" TEXT="tf-idf">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      http://en.wikipedia.org/wiki/Tf-idf
    </p>
    <p>
      
    </p>
    <p>
      This is a commonly used scheme in the vector space model.
    </p>
    <p>
      
    </p>
    <p>
      term frequency-inverse document frequency model, where weights in v_d = [w1,d, ..., wn,d] are assigned according to the tf-idf formula.
    </p>
    <p>
      
    </p>
  </body>
</html></richcontent>
</node>
</node>
<node CREATED="1392394734467" ID="ID_989860670" MODIFIED="1393809588755" POSITION="left" TEXT="papers">
<node CREATED="1392394736831" FOLDED="true" ID="ID_994498686" MODIFIED="1393085612393" TEXT="Concept Search: Semantics Enabled Information Retrieval***">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      This paper had a lot of useful information about the discipline of information retrieval in general. It gave me an appreciation for the depth of this field. I have since began pouring over more sources of information about it.
    </p>
    <p>
      
    </p>
    <p>
      A lot of their work can be applied to our bloom filter representation, e.g., instead of inserting &quot;dog&quot; insert &quot;dog-1&quot;, such that the correct sense of the word (if dog-1 is the correct sense) is used. If the user expresses an information need for a particular sense, then we won't mismatch due to synonymy (we can improve our precision without losing recall). We can go further, e.g., when seeing dog-1, also insert carnivore-1. And so on. A lot of possibilities, but some of their algorithmic sophistication doesn't seem to be able to work with out bloom filter document representation.
    </p>
    <p>
      
    </p>
    <p>
      A lot of notes are on the paper, but here are some issues off the top of my head...
    </p>
    <p>
      
    </p>
    <p>
      (1) IR: map a query to a set of ranked (ordered) documents according to their relevance to the query; the query represents the information need of the user.
    </p>
    <p>
      
    </p>
    <p>
      (2) They covered a lot of different relevancy metrics. Initially, I was thinking in terms of &quot;degree of matching to the query&quot; but I'm persuaded that &quot;relevance&quot; is a better way to think about this; also, a large body of existing work has already been established for doing this.
    </p>
    <p>
      &#160;&#160;&#160;&#160;(a) We'll be using some of the relevance metrics, but combine it with our own peculiar needs, e.g., we're also ranking blocks (not just documents) within a document, and since we can't see what the blocks contain exactly, we provide a useful visualization feature to show things like density or how close it is to the weighted keyword (or key terms) in the query.
    </p>
    <p>
      
    </p>
    <p>
      (3) They cover the main models: vector space, boolean, and probabilistic. Ours approach borrows a bit from these fundamental categories.
    </p>
    <p>
      &#160;&#160;&#160;&#160;(a) tf-idf, etc.
    </p>
    <p>
      
    </p>
    <p>
      (4) At detph, they cover the advantages/disadvantages with syntactic search, e.g., polysemy and synonymy, related concepts, etc. I've made notes about this elsewhere.
    </p>
  </body>
</html></richcontent>
<node CREATED="1392394729656" ID="ID_1935665911" MODIFIED="1392744560853" TEXT="Semantic Search">
<node CREATED="1392394761783" ID="ID_1193182347" MODIFIED="1392394764125" TEXT="Concept Search"/>
<node CREATED="1392394834347" ID="ID_81998661" MODIFIED="1392394835144" TEXT="basis">
<node CREATED="1392394836586" ID="ID_1174651365" MODIFIED="1392394871242" TEXT="computation of semantic relations between concepts"/>
<node CREATED="1392394890968" ID="ID_1192568301" MODIFIED="1392394895290" TEXT="needs semantic information"/>
<node CREATED="1392395469662" ID="ID_564585840" MODIFIED="1392395472970" TEXT="natural language processing"/>
</node>
</node>
<node CREATED="1392394718401" ID="ID_1214400277" MODIFIED="1392394725796" TEXT="Syntactic Search">
<node CREATED="1392394830221" ID="ID_1903635544" MODIFIED="1392394831252" TEXT="basis">
<node CREATED="1392394780153" ID="ID_661150830" MODIFIED="1392394846657" TEXT="string similarity between words">
<node CREATED="1392395217347" ID="ID_1138205690" MODIFIED="1392395225960" TEXT="syntactic matching"/>
</node>
<node CREATED="1392394974724" ID="ID_1649493191" MODIFIED="1392394981433" TEXT="atomic elements">
<node CREATED="1392394982828" ID="ID_1805140345" MODIFIED="1392394983984" TEXT="words"/>
<node CREATED="1392394985098" ID="ID_1115640762" MODIFIED="1392395005644" TEXT="multi-word phrases">
<node CREATED="1392394988928" ID="ID_1926440287" MODIFIED="1392394989912" TEXT="ngrams"/>
</node>
</node>
</node>
<node CREATED="1392395393300" ID="ID_659299512" MODIFIED="1392395395610" TEXT="big problems">
<node CREATED="1392395396568" ID="ID_1295929565" MODIFIED="1392395400329" TEXT="polysemy"/>
<node CREATED="1392395401240" ID="ID_1649157527" MODIFIED="1392395412458" TEXT="synonymy"/>
</node>
<node CREATED="1392485084961" ID="ID_951326979" MODIFIED="1392485086958" TEXT="measures">
<node CREATED="1392485094225" ID="ID_75556777" MODIFIED="1392485097677" TEXT="cosine similiarity"/>
</node>
</node>
<node CREATED="1392395151422" ID="ID_1664193963" MODIFIED="1392395165564" TEXT="Information Retrieval">
<node CREATED="1392395169760" ID="ID_570077858" MODIFIED="1392395172194" TEXT="Semantic"/>
<node CREATED="1392395173839" ID="ID_1033768651" MODIFIED="1392395177475" TEXT="Syntactic"/>
</node>
<node CREATED="1392395424944" ID="ID_1602071978" MODIFIED="1392744083590" TEXT="relationship to our approach">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      We mostly use syntactic searching... but things like stemming (finding word roots) are somewhat related to concept searching.
    </p>
    <p>
      
    </p>
    <p>
      It is interesting to point out, though, that a lot of their work can be used in our work.
    </p>
  </body>
</html></richcontent>
</node>
</node>
<node CREATED="1392601700817" ID="ID_1765963544" MODIFIED="1392863145355" TEXT="Practical Techniques for Searches on Encrypted Data">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      This is one of the first papers on encrypted searching.
    </p>
    <p>
      
    </p>
    <p>
      Demonstrates: searching functionality (a single keyword) without any loss of data confidentiality. Security of their approach was rigorously proven.
    </p>
    <p>
      
    </p>
    <p>
      Example use case: A mobile user with limited bandwidth wants to retrieve all email containing the keyword &quot;Urgent&quot; from an untrusted mail server. This is trivial to do when the server knows the content of the data (no confidentiality), but they show how it can be done when the server does not know the contents of the data, nor the contents of the query. Data confidentiality and query confidentiality (they call it hidden queries). In addition, the system supports controlled searching, i.e., a server cannot search for a word without the user's authorization. Finally, they support query isolation; the server learns nothing more than the search result about the plain text. (?)
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392581211297" FOLDED="true" ID="ID_587633218" MODIFIED="1392862649855" TEXT="secure and privacy preserving keyword searching for cloud storage services">
<node CREATED="1392581300289" ID="ID_1668024998" MODIFIED="1392581301518" TEXT="privacy">
<node CREATED="1392581259889" ID="ID_1906699613" MODIFIED="1392581262302" TEXT="user query privacy"/>
<node CREATED="1392581251537" ID="ID_299643274" MODIFIED="1392581258958" TEXT="user data privacy"/>
</node>
<node CREATED="1392581306321" ID="ID_489423147" MODIFIED="1392581450667" TEXT="natural approach to privacy">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Encrypt all the data.
    </p>
    <p>
      
    </p>
    <p>
      E.g., use public key to encrypt email + keywords before sending it to CSP then send queries in the form of encrypted keywords to retrieve email.
    </p>
  </body>
</html></richcontent>
<node CREATED="1392581450641" ID="ID_312732014" MODIFIED="1392581722592" TEXT="problems">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      CSP cannot determine which files meet information need of user, therefore must send all files.
    </p>
    <p>
      
    </p>
    <p>
      Decrypting/decrypting is cpu/memory intensive. This is problematic especially for thin-client type devices, e.g., tablets or smart phones.
    </p>
    <p>
      
    </p>
    <p>
      Another issue not address by paper: not everyone can search it; you must have the key. If you have the key,&#160;&#160;you can read file exactly as it is given. What if you want to give people &quot;partial&quot; access in the form of looking up keywords or looking up n-grams? (All or nothing.)
    </p>
  </body>
</html></richcontent>
<node CREATED="1392581453025" ID="ID_52537930" MODIFIED="1392581461678" TEXT="cpu/memory intensive"/>
<node CREATED="1392581475265" ID="ID_325002139" MODIFIED="1392581491710" TEXT="bandwidth-heavy"/>
<node CREATED="1392581693793" ID="ID_1851056281" MODIFIED="1392581696477" TEXT="all-or-nothing"/>
</node>
</node>
<node CREATED="1392581363840" FOLDED="true" ID="ID_455431115" MODIFIED="1392862648154" TEXT="terminology">
<node CREATED="1392581367105" ID="ID_654270541" MODIFIED="1392581375166" TEXT="CSP (cloud service provider)">
<node CREATED="1392581375841" ID="ID_823076920" MODIFIED="1392581376782" TEXT="trusted"/>
<node CREATED="1392581378289" ID="ID_1993030469" MODIFIED="1392581380046" TEXT="untrusted"/>
</node>
<node CREATED="1392581738449" ID="ID_1318677186" MODIFIED="1392581748894" TEXT="Bilinear Diffie-Hellman (BDH)"/>
<node CREATED="1392581752417" ID="ID_1832269462" MODIFIED="1392581763214" TEXT="random oracle model"/>
<node CREATED="1392581773617" ID="ID_172947762" MODIFIED="1392581782446" TEXT="PEKS (public key encryption with keyword searching)"/>
<node CREATED="1392581878049" ID="ID_447170503" MODIFIED="1392581922750" TEXT="EPPKS (efficient privacy preserving keyword searching scheme)"/>
<node CREATED="1392582030001" FOLDED="true" ID="ID_333315632" MODIFIED="1392586259804" TEXT="math terms">
<node CREATED="1392582034065" ID="ID_1525440993" MODIFIED="1392582049518" TEXT="cyclic groups"/>
<node CREATED="1392582038289" ID="ID_1831782281" MODIFIED="1392582048430" TEXT="prime order"/>
<node CREATED="1392582052865" ID="ID_854978258" MODIFIED="1392582055789" TEXT="additive group"/>
<node CREATED="1392582056577" ID="ID_347725901" MODIFIED="1392582065070" TEXT="multiplicative group"/>
<node CREATED="1392581725265" ID="ID_62857366" MODIFIED="1392581737582" TEXT="Bilinear map"/>
<node CREATED="1392582090129" ID="ID_1367434711" MODIFIED="1392582092030" TEXT="bilinear"/>
<node CREATED="1392582105489" ID="ID_203639290" MODIFIED="1392582110606" TEXT="super-singular ellypic curves"/>
<node CREATED="1392582116193" ID="ID_529958503" MODIFIED="1392582122734" TEXT="abelian"/>
</node>
<node CREATED="1392586264385" ID="ID_72998134" MODIFIED="1392586268734" TEXT="SPKS"/>
<node CREATED="1392586444513" ID="ID_395740369" MODIFIED="1392586448414" TEXT="IND-CPA"/>
</node>
<node CREATED="1392585151201" ID="ID_800498426" MODIFIED="1392585801001" TEXT="relevancy to our research">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      These results may be combined.
    </p>
    <p>
      
    </p>
    <p>
      A disadvantage to our approach is that:
    </p>
    <p>
      
    </p>
    <p>
      (1) A user (any user) may, if they have the bloom filter, find out which words -- with the possibility of a false positive -- are in the encrypted document. This may or may not be desirable behavior. Data privacy.
    </p>
    <p>
      
    </p>
    <p>
      (2) Someone eavesdropping on your queries can determine what your are interested in. Query privacy.
    </p>
    <p>
      
    </p>
    <p>
      How can this be addressed? Before inserting a word into the bloom filter, encrypt it and insert that into the bloom filter instead.
    </p>
    <p>
      
    </p>
    <p>
      Encrypt(SecretKey, Keyword) -&gt; Keyword'
    </p>
    <p>
      
    </p>
    <p>
      Now, to query the document for keyword k, you must do the following:
    </p>
    <p>
      
    </p>
    <p>
      Does the bloom filter have Encrypt(SecretKey, k)? If it tests as a positive, then (with some degree of false positive probability) his question is answered.
    </p>
    <p>
      
    </p>
    <p>
      What about mapping Encrypt(PublicKey, Keyword) instead? The end result is still a randomized sequence of characters (encrypted keyword) that has been inserted into the bloom filter.
    </p>
  </body>
</html></richcontent>
</node>
</node>
<node CREATED="1392592709969" ID="ID_812950319" MODIFIED="1393255035920" TEXT="on indexing and information disclosure measure for efficient cryptograph query">
<node CREATED="1392592748225" ID="ID_517343079" MODIFIED="1393596229222" TEXT="similarity">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Seems to be based on an idea similar to what i proposed; namely, optimizing the structure (in our case, hash functions at a given bloom filter size) such that we try to minimize the probability of a false positive on probable n-grams.
    </p>
    <p>
      
    </p>
    <p>
      On the topic of probable n-grams, I have been considering how to do this. I have already implemented a solution which will take in whatever corpus you feed it and spit out a large tree of grams, with the ability to sample from it according to the word frequencies in log2(n) time (well, log2(n1) + log2(n2) + ... + log2(nk), where k is size of ngram we wish to sample). An ideally suited text corpus is what? I'm thinking a few different kinds. One is to sample from common sources of text, uniformly, to try to model the general distribution found &quot;out in the wild&quot;. Another source of text is the document repository the user can choose documents from; this has a highly discriminating property -- but it assumes a user will not try queries composed of words not found in the document store.
    </p>
    <p>
      
    </p>
    <p>
      However, I see a combination approach working. Get a list of words which represents the top 99.9%, or whatever, of words seen in the language. Now, learn to not have a probability of a false positive on these 1-grams below a specified threshold (still using information about probability of seeing a 1-gram to prefer to have a false negative on an improbable 1-gram than a probable 1-gram). Then, if desired, learn up to k-grams *only from text in the document store*. Now, here's the outline: for a query with a k-gram in it to be a true positive, then all the 1-grams the k-gram is composed of must be true positives. We have already taken care of the 1-grams sufficiently well, so do this as a sanity check. If the k-gram check fails, then it's not a true positive. If the k-gram check succeeds, then it may be a false positive. Now, see if the 1-gram check fails. If it does, then assume it was a false positive on the k-gram check. If both checks succeed, then assume the k-gram is a true positive.
    </p>
    <p>
      
    </p>
    <p>
      As an extension, we can also do a 2-gram, ..., (k-1)-gram checks on subsets of the k-gram term in the query. They all must succeed for the k-gram to have the possibility of being a true positive. However, this amounts to 2^k checks, and for large k this is not reasonable, e.g., k=6 -&gt; 2^6 = 64 checks. This doesn't seem too unreasonable since all of them are constant lookups, but 2^10 is 1024, 2^20 is over a million. However, we don't have to be exhaustive. We only want 'evidence' that the, say, 20-gram is a true positive. Checking just the 1-grams may be enough. Checking also a random sampling of other grams may be all we need, say, a constant 10 other variations.
    </p>
    <p>
      
    </p>
    <p>
      So, now we have a method of being pretty confident in a positive result being a true positive. If the 1-gram check succeeds, it's possible it's a false positive. It's still possible for the 2-gram checks to fail, if so, and so on until we get to the k-gram. At each stage, failure for a false positive is a possibility, so this is pretty interesting. And, of course, all of these 1-grams, 2-grams, ..., k-grams also permit exact matches up to k words for a term, which is nice in its own right.
    </p>
  </body>
</html>
</richcontent>
</node>
<node CREATED="1392594315808" ID="ID_103359057" MODIFIED="1392594321886" TEXT="information disclosure">
<node CREATED="1392594323313" ID="ID_544945238" MODIFIED="1392594462923" TEXT="query disclosure">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      An eavesdropper can see which keywords the user is interested in by examining the query.
    </p>
    <p>
      
    </p>
    <p>
      A solution to this problem for our approach: mentioned elsewhere, but encrypt terms then hash the encrypted terms. This takes care of both data disclosure and query disclosure.
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392594328833" ID="ID_1583970269" MODIFIED="1392594530089" TEXT="data disclosure">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      A person can query document for terms to get an idea about what it might contain, up to the granularity of the n-grams and block sizes.
    </p>
    <p>
      
    </p>
    <p>
      A possible solution for our approach: see query disclosure. Basically, encrypt terms before inserting into bloom filter.
    </p>
  </body>
</html></richcontent>
</node>
</node>
</node>
<node CREATED="1392644297361" ID="ID_329501959" MODIFIED="1392646345192" TEXT="Extremely Fast Text Feature Extraction  for Classification and Indexing">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      This paper doesn't deal much with our research, but its discussion on representing strings as a hash vector had some interesting things to say, namely, that this representation serves equally as well as the bag of words (strings) representation when learning a classifier on it. They proposed a hash function which is quite simple -- a simple bit-shift -- but acquires variety by changing how a character maps to an integer. (This can be randomized.) This, it turns out, has much better distribution properties than using the value of the character itself compared to other well-known hash functions, e.g., Java's hash function.
    </p>
    <p>
      
    </p>
    <p>
      With regards to our research, this may be an efficient way to represent hash functions: only one one (or maybe a couple), but with different character -&gt; integer mappings. One can use a form of stochastic gradient descent to change the mappings in an advantageous way (with respect to training/validation minimizing false positives). I think this will be fast.
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392646261953" ID="ID_522021896" MODIFIED="1393883078800" TEXT="Shared and Searchable Encrypted Data for Untrusted Servers">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      This has a lot of interesting features. I haven't had time to digest the material yet, but a few points:
    </p>
    <p>
      
    </p>
    <p>
      (1) They assume authorized users are fully trusted. This is a bold assumption, and doesn't cover tiered access. Tiered access, especially if many people are stakeholders, is sometimes more appropriate. A simple example of tiered access:let some users search using the capabilities exposed by the document, but not open the document itself. This is a trivial case in our bloom filter example, but our example doesn't have as strong a guarantee about security, e.g., what information can be gleamed from the bloom filter distribution of values in indices? A part of our work may be to learn hash functions which optimize on this parameter in addition to reducing false negatives.
    </p>
  </body>
</html></richcontent>
<node CREATED="1392652214305" ID="ID_1417013953" MODIFIED="1392652522116" TEXT="trusting a KMS server">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      A &quot;flaw&quot; in their design: they trust a key management server. What if the server is compromised? What if the server isn't trustworthy? They make the reasonable argument that a KMS is a lot less involved than a data storage server, e.g., less data needs to be secured and it's easier to maintain. However, it is a weak link in the chain: they're solution is only as secure as the weakest link.
    </p>
    <p>
      
    </p>
    <p>
      But why does this vulnerability exist? To assist in other interesting functionality, namely key revocation. That is, you can revoke a user's access to the encrypted documents without having to go through an expensive process of reencrypting the documents under a new encryption key. So, that seems very applicable, and there's no easy solution to this problem without, seemingly, giving the system this vulnerability.
    </p>
  </body>
</html></richcontent>
</node>
</node>
<node CREATED="1392651125505" FOLDED="true" ID="ID_1437614978" MODIFIED="1393809654114" TEXT="Secure Conjunctive Keyword Search Over Encrypted Data">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      Discussion on capabilities seems interesting. They point out that they may want the user to be able to search for a conjunction of two terms, A and B, but not, for instance, B by itself. This behavior is possible to implement with n-gram granularity as proposed in our research, but without as strong of a security/privacy model.
    </p>
    <p>
      
    </p>
    <p>
      They also name some terms I've been thinking about, e.g., query privacy (not let an eavesdropper know what you've been searching for) and data privacy. In our case, query privacy is possible by hashing the encrypted terms (using an asymmetric or symmetric key) instead of the plaintext. This also prevents unauthorized users from querying the document at all.
    </p>
    <p>
      
    </p>
    <p>
      Also, they only discuss keyword searching in terms of certain fields, e.g., an email in which the subject or the email sender's email address can be searched (has a search capability). It's interesting to point
    </p>
    <p>
      
    </p>
    <p>
      They at times seem to create straw man arguments for simpler solutions, e.g., assuming that for a conjunctive query on a document in which there are m searchable keywords, that this would require 2^m storage space -- exponential in the number of searchable keywords. This strikes me as a little dishonest though since its unlikely all M terms will be useful in a query, therefore only a subset of m, say, k &lt; 5, should be sufficient. Even more selectivity is possible, e.g., allow only search terms to be used in a conjunction, thus allowing very large conjunctive queries without the exponential cost, e.g., (M choose k) + (M choose k-1) + ... + (M choose 2) + 1 instead of 2^M, which has intractable space complexity with respect to M.
    </p>
    <p>
      
    </p>
    <p>
      Another example of this is when they decry the simple set intersection approach of conjunctive queries consisting of searching on keyword key1 and keyword key2 separately and only returning those documents which have both keywords. They make the argument that this exposes a lot of extra information about the encrypted document -- and it does. But, if the keyword queries are, for instance, encrypted and we do a search on the encrypted keywords instead, the server doesn't even know what its looking for -- it just knows which documents they map to.
    </p>
  </body>
</html></richcontent>
<node CREATED="1392647336337" ID="ID_1744738446" MODIFIED="1392647340316" TEXT="nomenclature">
<node CREATED="1392647341249" ID="ID_445287150" MODIFIED="1392647342302" TEXT="PIR"/>
<node CREATED="1392648230577" ID="ID_1751646442" MODIFIED="1392648241500" TEXT="bilinear decisional diffie-hellman (BDDH)"/>
</node>
<node CREATED="1392647766417" ID="ID_864965484" MODIFIED="1392651074903" TEXT="distinguishing capabilities">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      I'm not really clear on how fruitful this line of inquiry is. What if you can, given two documents with indistinguishable capabilities (they both have the same keywords, for instance), distinguish one document from the other? What is the significance of the information that is gained because of this? It is never clearly articulated. That's not to say this is not a useful thing to have, only that I wish they had done a better job selling it. Maybe I didn't grok enough of the math to appreciate their analysis.
    </p>
    <p>
      
    </p>
    <p>
      Also, while keywords aren't revealed, the fields that the keywords belong to are. This is a limitation of their work; that is, first, a keyword must have an associated and unique field (which is limiting), and perhaps worse, the field is exposed during the query. So, while an eavesdropper may not be able to see which emails you are interested in, they may be able to tell the distribution of unique email addresses that you search for. This by itself can reveal private information, e.g., the distribution may be similar tp the distribution they see from you in a less secure setting, and therefore be able to make the mapping that way, e.g., this encrypted document is an email from alex@nowhere.net.
    </p>
  </body>
</html></richcontent>
<node CREATED="1392647815953" ID="ID_1840136426" MODIFIED="1392647832798" TEXT="e-advantage"/>
<node CREATED="1392647926689" ID="ID_415423642" MODIFIED="1392647959405" TEXT="ICC (indistinguishability of cipher from cipher)">
<node CREATED="1392647972033" ID="ID_371852163" MODIFIED="1392647980992" TEXT="adverserial game"/>
</node>
</node>
<node CREATED="1392647760257" ID="ID_1919955598" MODIFIED="1392647765486" TEXT="capabilities"/>
<node CREATED="1392652188464" ID="ID_681144222" MODIFIED="1392652192766" TEXT="query privacy"/>
<node CREATED="1392652194145" ID="ID_1862545434" MODIFIED="1392652195854" TEXT="data privacy"/>
</node>
<node CREATED="1392651150513" ID="ID_1640214991" MODIFIED="1393809644806" TEXT="Adding Compression to BLock Addressing Inverted Indexes***">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      See notes on paper.
    </p>
    <p>
      
    </p>
    <p>
      A few key observations:
    </p>
    <p>
      
    </p>
    <p>
      (1) Some similarity to our work: proposed inverted indexing to blocks instead of word positions (to save space); once they find out which blocks a word is in, they do a sequential scan. We have blocks too, for localization (i.e., ranking blocks in an encrypted document based on how close it is to all the weighted keywords, or some other metric), but we have the opposite approach: we do a sequential (or parallel) scan through the blocks (not the words in a block), and do constant lookups within each block.
    </p>
    <p>
      
    </p>
    <p>
      (2) I liked their discussion on using a Huffman tree on words as the atomic unit (leaves in the tree) instead of bits. Also, using bytes at a time to index into the tree seemed interesting, but I'll have to re-read it: how are they doing a simple left or right decision at a branch with byets instead of bits? I think I know but will have to re-read. Anyway, I may use this structure in a different way than them: to make the tree gram, a tree such that the deeper you go, the larger the n-gram you have traversed. My implementation of this works well but takes a lot of space; this coding scheme can drastically reduce its memory footprint (ngrams are notoriously space hungry) and maybe speed it up a tiny bit (by having fewer bytes to compare less than to?).
    </p>
    <p>
      
    </p>
    <p>
      Otherwise, their discussion on compression was irrelevant to our research. Bloom filter is made up on just boolean array elements, and its size is a function not of word lengths or other matters, but number of members (unique words/ngrams) and probability of a false positive. Unique words/ngrams is a function of n-gram granularity, text/block size, and freedom of regular expressions (e.g., how many classes to represent--alpha, alphanumeric, numeric?--and how many wildcard classes can be used in a word; I've quantified space complexity for both of these though more work needs to be done to fully formalize it).
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392664222729" ID="ID_265114291" MODIFIED="1393809648614" TEXT="supervised semantic indexing"/>
<node CREATED="1392698784481" ID="ID_1572211024" MODIFIED="1393809651206" TEXT="Fuzzy Keyword Search over Encrypted Data in Cloud Computing***">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      There were a few issues in this paper, but overall I thought they had a simple (clever) solution to a thorny problem: approximate matching with up to k-edit errors while avoiding the combinatorial explosion. So, here's the idea: when inserting &quot;abc&quot;, also insert &quot;ab*&quot;, &quot;a*c&quot;, &quot;*bc&quot;, &quot;*abc&quot;, &quot;a*bc&quot;, &quot;ab*c&quot;, &quot;abc*&quot;.
    </p>
    <p>
      
    </p>
    <p>
      So if the user searches for &quot;abb&quot;, and no match is found, then try 1-edit errors, including &quot;ab*&quot;, which is indeed in there.
    </p>
    <p>
      
    </p>
    <p>
      I have extended this concept to cover a limited form of regular expressions with limited classes. Hopefully, with the use of a bloom filter and, reasonably, expecting higher false positives with the regular expression matching, we can include many such variations without blowing up the bloom filter size. I think I have a good handle on how to do this. So, for instance, let's say a doc contains &quot;abc&quot;. Then, in the bloom filter, we insert &quot;abc&quot;, &quot;ab*&quot;, &quot;a*c&quot;, &quot;a**&quot;, &quot;*bc&quot;, &quot;*b*&quot;, &quot;**c&quot;, &quot;***&quot;. 2^3. Exponential, true, but we can limit this, e.g., if a query term is of size n, then we only perform at max (n choose k) * 2^k variations. If k = 3, then (n choose 3) * 8 for every term, if we assume average term is n = 6, then 8*n!/3!(n-3)!=8*6!/3!3!=160, so for every word we insert 160 variations. I think we might be pleasantly surprised by how well the hashes deal with this constant size increase. And if this is too large, I have a lot of ideas about how to reign in things. Anyway, back to problem. So, say the user wants to search for any pattern matching &quot;ab*&quot;, and the document contains &quot;abc&quot;, well, this will match.
    </p>
    <p>
      
    </p>
    <p>
      We can extend this to multiple classes, e.g., * = alphanumeric, # = numeric, &amp; = alphabetical. Math is then: (n choose k) * 3^k. Now it's (n choose 3) * 27, so a big jump. Obviously there's a trade-off between space complexity and regular expression generality.
    </p>
    <p>
      
    </p>
    <p>
      For &quot;abc&quot; we can also insert &quot;*abc&quot;, &quot;**abc&quot;, &quot;*a*bc&quot;, &quot;*ab*c&quot;, &quot;*abc*&quot;, ..., &quot;abc**&quot;. Clearly, this grows quickly. But now, we can do more complicated regular-like expression searches. These searches are constant, too, but with space complexity that grows exponentially in theory. HOWEVER, we can trade space complexity for false positives. This will be an interesting trade-off to explore; combined with learning good hashes, we may get a lot of mileage out of this approach. ALSO: If a query has more than k asterisks, we can immediately reject it; we don't support larger than k wildcards. That does leave character sequences with &lt;= k asterisks. We can construct negative training examples of these -- maybe just a subset -- and go after a desired false positive probability. But I think a higher false positive probability is reasonable for regular-like expression matching (on encrypted keyword searching on encrypted data).
    </p>
  </body>
</html></richcontent>
</node>
</node>
<node CREATED="1392559933121" ID="ID_334429193" MODIFIED="1392559934350" POSITION="left" TEXT="Weka"/>
<node CREATED="1392564735232" FOLDED="true" ID="ID_156189085" MODIFIED="1393802703610" POSITION="left" TEXT="nomenclature">
<node CREATED="1392563399297" ID="ID_39884767" MODIFIED="1392563400734" TEXT="Polysemy"/>
<node CREATED="1392564753137" ID="ID_317200162" MODIFIED="1392564756510" TEXT="Document"/>
<node CREATED="1392564757569" ID="ID_795239434" MODIFIED="1392564778145" TEXT="information need">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      http://en.wikipedia.org/wiki/Information_need
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392571704561" ID="ID_1744012879" MODIFIED="1392571782775" TEXT="Text Retrieval Conference (TREC)">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      http://en.wikipedia.org/wiki/Text_Retrieval_Conference
    </p>
    <p>
      
    </p>
    <p>
      <font color="rgb(0, 0, 0)" face="sans-serif" size="13px"><span style="display: inline !important; font-weight: normal; letter-spacing: normal; word-spacing: 0px; font-variant: normal; text-indent: 0px; background-color: rgb(255, 255, 255); text-align: start; font-style: normal; text-transform: none; font-size: 13px; float: none; line-height: 19.200000762939453px; font-family: sans-serif; color: rgb(0, 0, 0); white-space: normal">&quot;Uniform scoring is performed so the systems can be fairly evaluated.&quot;</span></font>
    </p>
  </body>
</html></richcontent>
</node>
<node CREATED="1392574151025" ID="ID_1125461675" MODIFIED="1392574166384" TEXT="Lucene">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      http://lucene.apache.org/
    </p>
  </body>
</html></richcontent>
<node CREATED="1392574167968" ID="ID_1274271212" MODIFIED="1392574174014" TEXT="openrelevance">
<richcontent TYPE="NOTE"><html>
  <head>
    
  </head>
  <body>
    <p>
      http://lucene.apache.org/openrelevance/
    </p>
  </body>
</html></richcontent>
</node>
</node>
<node CREATED="1392576871137" ID="ID_1937250305" MODIFIED="1392576874958" TEXT="As We May Think"/>
<node CREATED="1392577238577" ID="ID_438183989" MODIFIED="1392577246254" TEXT="structured / semi-structure / structured"/>
</node>
</node>
</map>
